from langchain_community.document_loaders import PDFPlumberLoader
from langchain_chroma import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

class RAGPipeline:
    def __init__(self, llm, embeddings, pdf_path, vectorstore_path="./vectorstore"):
        """
        Initializes the RAGPipeline class with the necessary components for processing PDFs and generating answers.

        Args:
            llm: The language model to be used for answering questions.
            embeddings: The embedding model to create vector representations of document chunks.
            pdf_path: Path to the PDF file to process.
            vectorstore_path: Directory to store the vectorized document representations.
        """
        self.ollama_embeddings = embeddings
        self.llm = llm

        # Load the PDF and extract its contents
        self.loader = PDFPlumberLoader(pdf_path)
        self.docs = self.loader.load()

        # Split the document into smaller chunks for easier processing
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=0
        )
        self.all_splits = self.text_splitter.split_documents(self.docs)

        # Load the predefined prompt template
        self.prompt = hub.pull("rlm/rag-prompt")

        # Format the documents into a single string for retrieval
        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in self.all_splits)

        # Create the vectorstore for efficient document retrieval
        self.vectorstore = self.create_vectorstore(
            documents=self.all_splits,
            embeddings=self.ollama_embeddings,
            persist_directory=vectorstore_path
        )

        # Define the question-answering chain
        self.qa_chain = (
            {
                "context": self.vectorstore.as_retriever() | format_docs,  # Retrieve and format relevant document chunks
                "question": RunnablePassthrough(),  # Pass the question directly to the next step
            }
            | self.prompt  # Add the question to the prompt template
            | self.llm  # Use the language model to generate the response
            | StrOutputParser()  # Parse the output into a string
        )

    def create_vectorstore(self, documents, embeddings, persist_directory="./vectorstore"):
        """
        Creates and returns a Chroma vectorstore from the provided documents and embeddings.

        Args:
            documents: List of document chunks to be vectorized.
            embeddings: The embedding model to use for creating vector representations.
            persist_directory: Directory to store the vectorized data.

        Returns:
            A Chroma vectorstore instance.
        """
        return Chroma.from_documents(
            documents=documents,
            embedding=embeddings,
            persist_directory=persist_directory
        )

    async def generate_stream(self, question: str):
        """
        Generates a streaming response for a given question.

        Args:
            question: The question to be answered.

        Yields:
            Chunks of the answer as they are generated by the language model.
        """
        stream = self.qa_chain.astream_events(question, version="v2")
        async for event in stream:
            kind = event["event"]  # Get the type of event
            if kind == "on_chat_model_stream":
                content = event["data"]["chunk"].content  # Extract content from the event
                if content:
                    yield content  # Yield the content chunk